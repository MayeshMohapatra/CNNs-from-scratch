{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing RCNN from scratch using torch and torchvision libaries\n",
    "#Importing the required libraries\n",
    "import torch\n",
    "import torchvision # for datasets and models\n",
    "import torchvision.transforms as transforms # for transforms\n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.nn.functional as F # for neural networks\n",
    "import torch.optim as optim # for optimizers\n",
    "import torchmetrics # for metrics\n",
    "from torchmetrics.classification.accuracy import Accuracy # for accuracy\n",
    "from torchvision.models import EfficientNet # for efficientnet backbone\n",
    "import cv2\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os # for os related operations\n",
    "import urllib # for downloading the dataset\n",
    "import tarfile # for extracting the dataset\n",
    "import xml.etree.ElementTree as ET # for parsing the xml files for annotations\n",
    "import pickle\n",
    "import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyper parameters beforehand\n",
    "config={'image_size':224, 'n_classes':21, 'bbox_reg': True, 'network': 'efficientnet-b0', 'max_proposals':2000, 'pad': 16}\n",
    "train_config={'log_wandb':True, 'logging': ['plot'],\n",
    "            'epochs': 3, 'batch_size':128, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "train_config_classifer={'log_wandb':True, 'logging': ['plot'],\n",
    "            'epochs': 1, 'batch_size':128, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "voc_2012_classes=['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Create Directory if it is not yet created\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset():\n",
    "    def __init__(self):\n",
    "        self.train_dir = None\n",
    "        self.test_dir = None\n",
    "        self.trainDataLink = None\n",
    "        self.testDataLink = None\n",
    "        \n",
    "        self.common_init()\n",
    "\n",
    "    # init that must be shared among all subclasses of this method\n",
    "    def common_init(self):\n",
    "        self.label_type=['none','aeroplane',\"Bicycle\",'bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'cow',\"Diningtable\",\"Dog\",\"Horse\",\"Motorbike\",'person', \"Pottedplant\",'sheep',\"Sofa\",\"Train\",\"TVmonitor\"]\n",
    "        self.convert_id=['background','Aeroplane',\"Bicycle\",'Bird',\"Boat\",\"Bottle\",\"Bus\",\"Car\",\"Cat\",\"Chair\",'Cow',\"Dining table\",\"Dog\",\"Horse\",\"Motorbike\",'Person', \"Potted plant\",'Sheep',\"Sofa\",\"Train\",\"TV/monitor\"]\n",
    "        self.convert_labels={}\n",
    "        for idx, x in enumerate(self.label_type):\n",
    "            self.convert_labels[x.lower()]=idx\n",
    "\n",
    "        self.num_classes=len(self.label_type) # 20 + 1(none)\n",
    "    \n",
    "    # download the dataset\n",
    "    def download_dataset(self,validation_size=5000):\n",
    "        \"\"\"\n",
    "        Downloads the dataset and extracts it\n",
    "        \"\"\"\n",
    "        #print(\"[@] Downloading dataset [@]\") \n",
    "        #print(self.trainDataLink) # print the link \n",
    "        #urllib.request.urlretrieve(self.trainDataLink, 'voctrain.tar') # type: ignore # download the dataset\n",
    "\n",
    "        #print(\"[@] Extracting dataset [@]\")\n",
    "        tar = tarfile.open(\"voctrain.tar\",\"r:\") # open the tar file\n",
    "        tar.extractall('RCNN\\\\voctrain.tar') # extract the tar file\n",
    "        tar.close() # close the tar file\n",
    "        #os.remove('content/voctrain.rar') # remove the tar file\n",
    "\n",
    "        if self.testDataLink is None: \n",
    "            # Move 5k images to validation state\n",
    "            print(\"[@] Moving 5k images to validation state [@]\")\n",
    "            print(self.test_dir)\n",
    "            ensure_dir(self.test_dir+'\\\\Annotations\\\\') # type: ignore # create the directory if it doesnt exist\n",
    "            ensure_dir(self.test_dir+'\\\\JPEGImages\\\\') # type: ignore # create the directory if it doesnt exist\n",
    "\n",
    "            random.seed(42) # set the seed\n",
    "            print(len(os.listdir(self.train_dir+'\\\\JPEGImages\\\\'))) # print the number of images in the train directory\n",
    "            val_images = random.sample(sorted(os.listdir(self.train_dir+'\\\\JPEGImages\\\\')),validation_size) # type: ignore # get the random images for validation set\n",
    "\n",
    "            #for path in val_images:\n",
    "             #   img_name = path.split('/')[-1].split('.')[0] # get the image name\n",
    "                # move images\n",
    "                #os.rename(self.train_dir+'/JPEGImages/'+img_name+'.jpg',self.test_dir+'/JPEGImages/'+img_name+'.jpg')  # type: ignore\n",
    "                # move anotations\n",
    "                #os.rename(self.train_dir+'/Annotations/'+img_name+'.xml',self.test_dir+'/Annotations/'+img_name+'.xml')  # type: ignore\n",
    "        else:\n",
    "            # Load the validation data\n",
    "            print(\"[@] Downloading validation dataset [@]\")\n",
    "            urllib.request.urlretrieve(self.testDataLink, 'voctest.tar') # type: ignore # download the dataset\n",
    "\n",
    "            print(\"[@] Extracting validation dataset [@]\")\n",
    "            tar = tarfile.open(\"voctest.tar\",\"r:\") # open the tar file\n",
    "            tar.extractall('/content/VOCtest') # extract the tar file\n",
    "            tar.close() # close the tar file\n",
    "            os.remove('content/voctest.rar') # remove the tar file\n",
    "\n",
    "    def read_xml(self,xml_path):\n",
    "        \"\"\"\n",
    "        Read the xml file and return the data\n",
    "        Args:\n",
    "            xml_path: path to the xml file\n",
    "        \"\"\"\n",
    "        object_list = [] \n",
    "\n",
    "        tree = ET.parse(open(xml_path,'r')) # parse the xml file\n",
    "        root = tree.getroot() # get the root of the xml file\n",
    "\n",
    "        objects = root.findall('object')\n",
    "        for _object in objects:\n",
    "            name = _object.find(\"name\") # get the name of the object\n",
    "            bndbox = _object.find(\"bndbox\") # get the bounding box of the object\n",
    "            # get the coordinates of the bounding box\n",
    "            xmin = int(bndbox.find(\"xmin\"))  # type: ignore\n",
    "            ymin = int(bndbox.find(\"ymin\"))  # type: ignore\n",
    "            xmax = int(bndbox.find(\"xmax\"))  # type: ignore\n",
    "            ymax = int(bndbox.find(\"ymax\"))  # type: ignore\n",
    "            class_name = _object.find(\"name\") # get the class name of the object\n",
    "            object_list.append({'x1':xmin, 'x2':xmax, 'y1':ymin, 'y2':ymax, 'class':self.convert_labels[class_name]}) # append the object to the list\n",
    "        return object_list # return the list of objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007(VOCDataset):\n",
    "    # Class for the VOC2007 dataset\n",
    "    def __init__(self):\n",
    "        self.train_dir = '/content/VOCtrain/VOCdevkit/VOC2007'\n",
    "        self.test_dir = '/content/VOCtest/VOCdevkit/VOC2007'\n",
    "        self.trainDataLink = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar'\n",
    "        self.testDataLink = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar'\n",
    "        self.common_init()\n",
    "\n",
    "class VOC2012(VOCDataset):\n",
    "    # Class for the VOC2012 dataset\n",
    "    def __init__(self):\n",
    "        self.train_dir = 'RCNN\\voctrain\\VOCdevkit\\VOC2012'\n",
    "        self.test_dir = 'RCNN\\voctrain\\VOCdevkit\\VOC2012'\n",
    "        self.trainDataLink = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'\n",
    "        self.testDataLink = None\n",
    "        self.common_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'RCNN\\x0boctrain.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m voc_dataset\u001b[39m=\u001b[39mVOC2012() \u001b[39m# create the dataset object\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m voc_dataset\u001b[39m.\u001b[39;49mdownload_dataset() \u001b[39m# download the dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [16], line 31\u001b[0m, in \u001b[0;36mVOCDataset.download_dataset\u001b[1;34m(self, validation_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m#print(\"[@] Downloading dataset [@]\") \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#print(self.trainDataLink) # print the link \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#urllib.request.urlretrieve(self.trainDataLink, 'voctrain.tar') # type: ignore # download the dataset\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m#print(\"[@] Extracting dataset [@]\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m tar \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mvoctrain.tar\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr:\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# open the tar file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m tar\u001b[39m.\u001b[39;49mextractall(\u001b[39m'\u001b[39;49m\u001b[39mRCNN\u001b[39;49m\u001b[39m\\v\u001b[39;49;00m\u001b[39moctrain.tar\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# extract the tar file\u001b[39;00m\n\u001b[0;32m     32\u001b[0m tar\u001b[39m.\u001b[39mclose() \u001b[39m# close the tar file\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m#os.remove('content/voctrain.rar') # remove the tar file\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\tarfile.py:2045\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[0;32m   2043\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[0;32m   2044\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[1;32m-> 2045\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[0;32m   2046\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[0;32m   2048\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[0;32m   2049\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\tarfile.py:2086\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[1;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2083\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[0;32m   2085\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2086\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m   2087\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[0;32m   2088\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[0;32m   2089\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2090\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\tarfile.py:2151\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2147\u001b[0m upperdirs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(targetpath)\n\u001b[0;32m   2148\u001b[0m \u001b[39mif\u001b[39;00m upperdirs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(upperdirs):\n\u001b[0;32m   2149\u001b[0m     \u001b[39m# Create directories that are not part of the archive with\u001b[39;00m\n\u001b[0;32m   2150\u001b[0m     \u001b[39m# default permissions.\u001b[39;00m\n\u001b[1;32m-> 2151\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(upperdirs)\n\u001b[0;32m   2153\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39mislnk() \u001b[39mor\u001b[39;00m tarinfo\u001b[39m.\u001b[39missym():\n\u001b[0;32m   2154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m -> \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tarinfo\u001b[39m.\u001b[39mname, tarinfo\u001b[39m.\u001b[39mlinkname))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[0;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'RCNN\\x0boctrain.tar'"
     ]
    }
   ],
   "source": [
    "voc_dataset=VOC2012() # create the dataset object\n",
    "voc_dataset.download_dataset() # download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datalen=len(os.listdir(voc_dataset.test_dir+'/Annotations'))\n",
    "train_datalen=len(os.listdir(voc_dataset.train_dir+'/Annotations'))\n",
    "print(train_datalen, val_datalen) # 17,125 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(image,title=\"image\"):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_name = sorted(os.listdir(voc_dataset.train_dir+'/JPEGImages'))[0][:-4]\n",
    "\n",
    "img=cv2.imread(voc_dataset.train_dir+'/JPEGImages/'+img_name+'.jpg')\n",
    "cv2_imshow(img)\n",
    "print('Shape:', img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced code of https://deepbaksuvision.github.io/Modu_ObjectDetection/posts/02_01_PASCAL_VOC.html\n",
    "xml_path=voc_dataset.train_dir+'/Annotations/'+img_name+'.xml'\n",
    "tree = ET.parse(open(xml_path, 'r'))\n",
    "\n",
    "root=tree.getroot()\n",
    "# print image shape\n",
    "w, h = root.find(\"size\").find(\"width\").text, root.find(\"size\").find(\"height\").text\n",
    "print('w, h:', w, h)\n",
    "\n",
    "# plot bounding boxes\n",
    "box_im=img.copy()\n",
    "bbox_color=(0, 0, 255) # (b, g, r) not (r, g, b)\n",
    "bbox_thickness=2\n",
    "\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale              = 0.5\n",
    "fontColor              = bbox_color\n",
    "lineType               = 1\n",
    "\n",
    "\n",
    "objects = root.findall(\"object\")\n",
    "for _object in objects:\n",
    "  name = _object.find(\"name\").text\n",
    "  bndbox = _object.find(\"bndbox\")\n",
    "  xmin = int(bndbox.find(\"xmin\").text)\n",
    "  ymin = int(bndbox.find(\"ymin\").text)\n",
    "  xmax = int(bndbox.find(\"xmax\").text)\n",
    "  ymax = int(bndbox.find(\"ymax\").text)\n",
    "  class_name = _object.find('name').text\n",
    "\n",
    "  cv2.rectangle(box_im, (xmin, ymin), (xmax, ymax), bbox_color, bbox_thickness)\n",
    "  cv2.putText(box_im, class_name, (xmin, ymin-5), font, \n",
    "    fontScale,\n",
    "    fontColor,\n",
    "    lineType)\n",
    "\n",
    "cv2_imshow(box_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Demonstrating Selective Search Algorithm using opencv\"\"\"\n",
    "img_ss=img.copy() # copy the image\n",
    "\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation() # create the selective search object\n",
    "ss.setBaseImage(img_ss) # set the base image\n",
    "ss.switchToSelectiveSearchFast() # \"... extract around 2000 region proposals (we use selective search’s “fast mode” in all experiments).\"\n",
    "rects = ss.process()\n",
    "\n",
    "\n",
    "print('Found',len(rects),'boxes...')\n",
    "for i, rect in (enumerate(rects)):\n",
    "    if i>2000:\n",
    "        break\n",
    "    x, y, w, h = rect\n",
    "    cv2.rectangle(img_ss, (x, y), (x+w, y+h), (100, 255, 100), 1)\n",
    "    \n",
    "cv2_imshow(img_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions for selective search and plotting results\"\"\"\n",
    "\n",
    "def selective_search(image):\n",
    "    \"\"\"Perform selective search on the image and return the regions\n",
    "    Parameters:\n",
    "        image: the image to perform selective search on\n",
    "    Returns:\n",
    "        rects: the regions of interest\n",
    "    \"\"\"\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    return rects \n",
    "\n",
    "def plot_results(image, bboxes, color = (0,69,255)):\n",
    "    \"\"\"\n",
    "    Plot the bounding boxes on the image\n",
    "    Parameters:\n",
    "        image: the image to plot on\n",
    "        bboxes: the bounding boxes to plot\n",
    "        color: the color of the bounding boxes\n",
    "    \n",
    "    Returns:\n",
    "        image: the image with the bounding boxes plotted on it\n",
    "    \"\"\"\n",
    "    plot_cfg = {\n",
    "        'bbox_color': color, 'bbox_thickness': 2,\n",
    "        'font': cv2.FONT_HERSHEY_SIMPLEX, 'font_scale': 0.5,\n",
    "        'font_color': color, 'lineThickness': 1\n",
    "    }\n",
    "    img_ss = image.copy()\n",
    "    for box in bboxes:\n",
    "        bbox = box['bbox']\n",
    "        cv2.rectangle(img_ss, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), plot_cfg['bbox_color'], plot_cfg['bbox_thickness'])\n",
    "        cv2.putText(img_ss, f\"{voc_dataset.label_type[box['class']]}, {str(box['conf'])[:5]}\",  (bbox['x1'], bbox['y1'] - 5), plot_cfg['font'], \n",
    "                plot_cfg['fontScale'], plot_cfg['fontColor'], plot_cfg['lineThickness'])\n",
    "    return img_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper function to calculate IoU\"\"\"\n",
    "\n",
    "def calculate_IoU(bb1,bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "    IoU == Area of Overlap / Area of Union\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : dict\n",
    "        Keys: 'x1', 'x2', 'y1', 'y2'\n",
    "    bb2 : dict\n",
    "        Keys: 'x1', 'x2', 'y1', 'y2'\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "    x_left = max(bb1['x1'], bb2['x1']) # max of left x-coord\n",
    "    y_top = max(bb1['y1'], bb2['y1']) # max of top y-coord \n",
    "    x_right = min(bb1['x2'], bb2['x2']) # min of right x-coord\n",
    "    y_bottom = min(bb1['y2'], bb2['y2']) # min of bottom y-coord\n",
    "\n",
    "    # For no overlap, output should be 0\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate the overlap area\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top) # Area of overlap/intersection\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1']) # area of bb1\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1']) # Area of bb2\n",
    "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
    "    union_area = bb1_area + bb2_area - intersection_area # Union = Total Area - Overlap Area\n",
    "\n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to calculate mean average precision (mAP)\"\"\"\n",
    "def mean_average_precision(pred, truth, iou_threshold=0.5, num_classes = 21, per_class = False):\n",
    "    \"\"\"\n",
    "    Calculate the mean average precision (mAP) of the model predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : list\n",
    "        A list of dictionaries containing the model predictions.\n",
    "    truth : list\n",
    "        A list of dictionaries containing the ground truth bounding boxes.\n",
    "    iou_threshold : float\n",
    "        The threshold for the Intersection over Union (IoU) score.\n",
    "    num_classes : int\n",
    "        The number of classes in the dataset.\n",
    "    per_class : bool\n",
    "        Whether to calculate the mAP for each class.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean average precision (mAP) of the model predictions.\n",
    "    \"\"\"\n",
    "    # Calculate the average precision for each class\n",
    "    average_precisions = []\n",
    "\n",
    "    epsilon = 1e-6 # A small number to avoid division by zero   \n",
    "\n",
    "    for c in range(num_classes):\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        total_true_bboxes = 0\n",
    "\n",
    "        # List detected(predicted) objects for class 'c'\n",
    "        detections = []\n",
    "\n",
    "        for idx, prs in enumerate(pred): # loop through all the predictions\n",
    "            for pr in prs: # loop through all the predictions for a single image\n",
    "                if pr['class'] == c: # if the prediction is for class 'c'\n",
    "                    detections.append((pr['conf'],idx, pr['bbox'])) # (confidence, image_idx, bbox)\n",
    "                    #detections.append({'conf': pr['conf'], 'idx': idx, 'bbox': pr['bbox']})\n",
    "        total_true_bboxes = 0 # Total number of ground truth objects for class 'c'\n",
    "        is_detected = [] # List of booleans indicating if a ground truth bbox has been detected\n",
    "        for gts in pred: # Loop through ground truth objects\n",
    "            is_detected.append([False for _ in gts]) # List of booleans for each ground truth bounding box\n",
    "            total_true_bboxes += sum([gt['class']==c for gt in gts]) # Count the number of ground truth bounding boxes for class 'c'\n",
    "        # Sort the detections by confidence score\n",
    "        detections.sort(key=lambda x: x[0], reverse=True) # key = lambda x: x[0] is the same as key = itemgetter(0)\n",
    "\n",
    "        TP = torch.zeros((len(detections))) \n",
    "        FP = torch.zeros((len(detections)))\n",
    "\n",
    "        if total_true_bboxes == 0:\n",
    "            continue \n",
    "        \n",
    "        # Calculate the average precision \n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            #Only take ground truths that have the same training index as the detection\n",
    "            num_gts = len(truth[detection[1]])\n",
    "\n",
    "            # find most closest ground truth box to pred as best_gt_index\n",
    "            best_iou = 0\n",
    "            best_gt_index = 0\n",
    "\n",
    "            for idx, gt in enumerate(truth[detection[1]]): # for each ground truth box\n",
    "                iou = calculate_IoU(gt,detection[2]) # calculate IoU\n",
    "                if iou > best_iou: # if iou is better than previous best iou\n",
    "                    best_iou = iou\n",
    "                    best_gt_index = idx\n",
    "\n",
    "            # If the IoU is higher than the threshold set this detection as true positive\n",
    "            if best_iou > iou_threshold:\n",
    "                # Only detect ground truth detection once\n",
    "                if is_detected[detection[1]][best_gt_index] == False: # if ground truth box is not detected\n",
    "                    # Set the flag to true\n",
    "                    TP[detection_idx] = 1\n",
    "                    is_detected[detection[1]][best_gt_index] = True\n",
    "                else:\n",
    "                    # Duplicate is FP\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                # If the IoU is lower than the threshold set this detection as false positive\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        # Calculate the cumulative sum of the true positives and false positives\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "\n",
    "        # Calculate the recall and precision\n",
    "        recall = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precision = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "\n",
    "        # Calculate the average precision\n",
    "        precisions = torch.cat((torch.tensor([1]), precision))\n",
    "        recalls = torch.cat((torch.tensor([0]), recall))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "        if per_class:\n",
    "            return average_precisions\n",
    "        else:\n",
    "            return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementing Non-Max Suppression\"\"\"\n",
    "\n",
    "def nms(p,iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform non-max suppression on the predictions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : list\n",
    "        A list of dictionaries containing the model predictions.\n",
    "        {'bbox': (x1, y1, x2, y2), 'class': class, 'conf': confidence}\n",
    "    iou_threshold : float\n",
    "        The threshold for the Intersection over Union (IoU) score.\n",
    "    Returns \n",
    "    -------\n",
    "    list\n",
    "        A list of dictionaries containing the model predictions after performing non-max suppression.\n",
    "    \"\"\"\n",
    "    conf_list = np.array([x['conf'] for x in p]) # List of confidence scores\n",
    "    conf_order = (-conf_list).argsort() # Sort the predictions by confidence score in descending order\n",
    "    isremoved = [False for _ in p] # List of booleans indicating if a prediction has been removed\n",
    "    keep = []\n",
    "\n",
    "    for idx in range(len(p)):\n",
    "        to_keep = conf_order[idx]\n",
    "        if isremoved[to_keep] == False:\n",
    "            continue\n",
    "        # Add the prediction with the highest confidence score to the list of predictions to keep\n",
    "        keep.append(p[to_keep])\n",
    "        isremoved[to_keep] = True\n",
    "\n",
    "        # Removing overlapping boxes\n",
    "        for order in range(idx+1, len(p)):\n",
    "            bbox_idx = conf_order[order]\n",
    "            if isremoved[bbox_idx] == False:\n",
    "                # check overlap\n",
    "                iou = calculate_IoU(p[to_keep]['bbox'],p[bbox_idx]['bbox'])\n",
    "                if iou > iou_threshold:\n",
    "                    isremoved[bbox_idx] = True\n",
    "    return keep\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain specific fine-tuning the dataset\n",
    "\"\"\"Implementing the dataset class\"\"\"\n",
    "class RCNN_Dataset(torch.utils.data.Dataset):\n",
    "    # Dataset class for the RCNN model\n",
    "    def __init__(self, dataset, cfg, IOU_threshold={'positive':0.5, 'partial':0.3}, sample_ratio=(32,96),data_path='data'):\n",
    "    \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : list\n",
    "            A list of dictionaries containing the dataset.\n",
    "        cfg : dict\n",
    "            A dictionary containing the configuration parameters.\n",
    "        IOU_threshold : dict\n",
    "            A dictionary containing the IOU thresholds for the positive and partial samples.\n",
    "        sample_ratio : tuple\n",
    "            A tuple containing the ratio of positive and partial samples.\n",
    "        data_path : str\n",
    "            The path to the data folder.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.cfg = cfg\n",
    "        self.IOU_threshold = IOU_threshold\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.transform = transforms.Compose([transforms.Resize((cfg['image_size'], cfg['image_size'])), transforms.ToTensor(), transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
    "        self.data_path = data_path\n",
    "\n",
    "        if self.dataset_exists() == False:\n",
    "            self.generate_dataset(sample_ratio, IOU_threshold)\n",
    "        else:\n",
    "            print(\"Loading Dataset\")\n",
    "            with open(self.data_path+'train_images.pkl', 'rb') as f:\n",
    "                self.train_images = pickle.load(f)\n",
    "            with open(self.data_path+'train_labels.pkl', 'rb') as f:\n",
    "                self.train_labels = pickle.load(f)\n",
    "            \n",
    "            if not len(self.train_images) == len(self.train_labels):\n",
    "                raise ValueError(\"The number of images and labels do not match\")\n",
    "            print(\"Dataset loaded\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sample to return.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the image and the labels.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() # Convert to list if tensor\n",
    "            image = Image.fromarray(cv2.cvtColor(self.train_images[idx],cv2.COLOR_BGR2RGB)) # Convert to PIL image\n",
    "            return {'image': self.transform(image), 'label': self.train_labels[idx][0], 'est_bbox': self.train_labels[idx][1], 'gt_bbox': self.train_labels[idx][2]}\n",
    "    \n",
    "    def dataset_exists(self):\n",
    "        # Check if the dataset has been generated\n",
    "        return os.path.exists(self.data_path+'train_images.pkl') and os.path.exists(self.data_path+'train_labels.pkl')\n",
    "    \n",
    "    def generate_dataset(self,sample_ratio, IoU_threshold, padding = 16):\n",
    "        \"\"\"\n",
    "        Generate the dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_ratio : tuple\n",
    "            A tuple containing the ratio of positive and partial samples.\n",
    "        IoU_threshold : dict\n",
    "            A dictionary containing the IOU thresholds for the positive and partial samples.\n",
    "        padding : int\n",
    "            The padding to be added to the bounding box.\n",
    "        \"\"\"\n",
    "        image_dir = self.dataset.train_dir + 'JPEGImages/'\n",
    "        annot_dir = self.dataset.train_dir + 'Annotations/'\n",
    "        obj_counter = 0\n",
    "        bg_counter = 0\n",
    "        self.train_images = [] # List of images\n",
    "        self.train_labels = [] # List of labels\n",
    "\n",
    "        print(\"Generating Dataset for RCNN\")\n",
    "\n",
    "        pbar = tqdm(sorted(os.listdir(image_dir))[:2000], position = 0, leave = True) # Progress bar\n",
    "\n",
    "        for img_name in pbar:\n",
    "            pbar.set_description(f\"Data Size : {len(self.train_labels)}\")\n",
    "            image = cv2.imread(image_dir + img_name) # Read the image\n",
    "            xml_path = annot_dir + img_name[:-4] + '.xml' # Get the path to the xml file\n",
    "            gt_bboxes = self.dataset.get_bbox(xml_path) # Get the ground truth bounding boxes\n",
    "            rects = selective_search(image)[:2000] # Get the selective search regions of interest (ROI)\n",
    "            random.shuffle(rects) # Shuffle the regions\n",
    "\n",
    "            for x,y,w,h in rects: # Iterate through the regions\n",
    "                x1,x2 = np.clip([x-padding, x+w+padding], 0, image.shape[1]) # Add padding to the bounding box\n",
    "                y1,y2 = np.clip([y-padding, y+h+padding], 0, image.shape[0]) # Add padding to the bounding box\n",
    "                bbox_est = {'x1':x1, 'x2':x2, 'y1':y1, 'y2':y2} # Estimated bounding box\n",
    "\n",
    "                is_object = False # Flag to check if the region is an object\n",
    "                for gt_bbox in gt_bboxes: # Iterate through the ground truth bounding boxes\n",
    "                    iou = calculate_IoU(gt_bbox, bbox_est) # Calculate the IOU\n",
    "\n",
    "                    if iou > IoU_threshold['positive']: # If the IOU is greater than the positive threshold\n",
    "                        obj_counter += 1 # Increment the object counter\n",
    "                        cropped = image[bbox_est['y1']:bbox_est['y2'], bbox_est['x1']:bbox_est['x2'], :] # Crop the image to the bounding box region  \n",
    "                        self.train_images.append(cropped) # Append the cropped image to the list\n",
    "                        est_bbox_xywh = ((bbox_est['x1'] + bbox_est['x2'])/2, (bbox_est['y1'] + bbox_est['y2'])/2,\n",
    "                                        bbox_est['x2'] - bbox_est['x1'], bbox_est['y2'] - bbox_est['y1']) # Convert the bounding box to xywh format\n",
    "                        gt_bbox_xywh = ((gt_bbox['x1'] + gt_bbox['x2'])/2, (gt_bbox['y1'] + gt_bbox['y2'])/2, \n",
    "                                        gt_bbox['x2'] - gt_bbox['x1'], gt_bbox['y2'] - gt_bbox['y1']) # Convert the bounding box to xywh format\n",
    "                        self.train_labels.append([gt_bbox['class'], est_bbox_xywh, gt_bbox_xywh]) # Append the label to the list\n",
    "\n",
    "                        is_object = True # Set the flag to true\n",
    "                        break \n",
    "                    # if the object is not close to any gt box\n",
    "                    if bg_counter < sample_ratio[1] and is_object == False: # If the region is not an object and the background counter is less than the partial sample ratio\n",
    "                        bg_counter += 1 # Increment the background counter\n",
    "                        cropped = image[bbox_est['y1']:bbox_est['y2'], bbox_est['x1']:bbox_est['x2'], :]  # Crop the image to the bounding box region\n",
    "                        self.traim_images.append(cropped) # Append the cropped image to the list\n",
    "                        est_bbox_xywh = (1.0, 1.0, 1.0, 1.0) # Set the estimated bounding box to 1.0 (no object)\n",
    "                        gt_bbox_xywh = (1.0, 1.0, 1.0, 1.0) # Set the ground truth bounding box to 1.0 (no object)\n",
    "                        self.train_labels.append([0, est_bbox_xywh, gt_bbox_xywh]) # Append the label to the list\n",
    "                    \n",
    "                    if obj_counter >= sample_ratio[0] and bg_counter == sample_ratio[1]: # If the object counter is greater than the positive sample ratio and the background counter is greater than the partial sample ratio\n",
    "                        obj_counter -= sample_ratio[0] # Decrement the object counter\n",
    "                        bg_counter = 0 # Reset the background counter\n",
    "                    \n",
    "            print(\"Dataset Generated! Saving to\", \"self.data_path\")\n",
    "            with open(self.data_path+'train_images.pkl', 'wb') as f:\n",
    "                pickle.dump(self.train_images, f)\n",
    "            with open(self.data_path+'train_labels.pkl', 'wb') as f:\n",
    "                pickle.dump(self.train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN_DatasetLoader(voc_dataset, cfg, training_cfg, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a dataset loader for the RCNN.\n",
    "    Parameters\n",
    "    ----------\n",
    "    voc_dataset : VOC_Dataset\n",
    "        The VOC dataset.\n",
    "    cfg : dict\n",
    "        The configuration dictionary.\n",
    "    training_cfg : dict\n",
    "        The training configuration dictionary.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the dataset.\n",
    "    Returns\n",
    "    -------\n",
    "    dataset_loader : DataLoader\n",
    "        The dataset loader.\n",
    "    \"\"\"\n",
    "    ds = RCNN_Dataset(voc_dataset, cfg)\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=training_cfg['batch_size'], shuffle=shuffle, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'image_size':224, 'n_classes':21, 'bbox_reg': True, 'network': 'efficientnet-b0', 'max_proposals':2000, 'pad': 16}\n",
    "train_config={'log_wandb':True, 'logging': ['plot'],\n",
    "            'epochs': 3, 'batch_size':128, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "train_config_classifer={'log_wandb':True, 'logging': ['plot'],\n",
    "            'epochs': 1, 'batch_size':128, 'lr': 0.001, 'lr_decay':0.5, 'l2_reg': 1e-5}\n",
    "\n",
    "loader = RCNN_DatasetLoader(voc_dataset, config, train_config, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e284ee3255a07ad8bf76694974743c4c81cb57e7c969474d752d949b11d721e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
