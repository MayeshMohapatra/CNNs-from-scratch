{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "from collections import OrderedDict # For saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(101)\n",
    "\n",
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available. Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available! Training on GPU ...')\n",
    "\n",
    "# Defining the transition layers for the DenseNet which do the downsampling (1x1 convolution and pooling)\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(_Transition,self).__init__() # call the init function of the parent class\n",
    "        self.add_module('norm', nn.BatchNorm2d(in_channels)) # Batch Normalization is used to normalize the input layer by re-centering and re-scaling\n",
    "        self.add_module('relu', nn.ReLU(inplace=True)) # ReLU is used to introduce non-linearity in the network)\n",
    "        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)) # 1x1 convolution is used to reduce the number of channels\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2)) # Average pooling is used to downsample the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the dense layer inside the dense block\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient = False ):\n",
    "        \"\"\"\n",
    "        Function for initializing the dense layer\n",
    "        Args:\n",
    "            num_input_features (int) - how many input features\n",
    "            growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "            bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "                (i.e. bn_size * k features in the bottleneck layer)\n",
    "            drop_rate (float) - dropout rate after each dense layer\n",
    "            memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower.\n",
    "        \"\"\"\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, \n",
    "                                         bn_size * growth_rate,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1, bias=False\n",
    "                                        ))\n",
    "    \n",
    "    def bn_function(self, inputs):\n",
    "        \"\"\"\n",
    "        Function for bottleneck layer\n",
    "        Args:\n",
    "            inputs (torch.autograd.Variable) - input to the layer\n",
    "        \"\"\"\n",
    "        concated_features = torch.cat(inputs, 1) # concatenate the input features\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features))) # apply the 1x1 convolution\n",
    "        return bottleneck_output\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Function for forward pass of the dense layer\n",
    "        Args:\n",
    "            input (torch.autograd.Variable) - input to the layer\n",
    "        \"\"\"\n",
    "        if isinstance(input, torch.Tensor): # if the input is a tensor\n",
    "            prev_features = [input] # store the input as a list\n",
    "        else:\n",
    "            prev_features = input # else store the input as a list\n",
    "        \n",
    "        bottleneck_output = self.bn_function(prev_features) # apply the bottleneck layer\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output))) # apply the 3x3 convolution\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) # apply dropout if drop_rate > 0\n",
    "        \n",
    "        return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a DenseBlock which consists of multiple DenseLayers\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2 # version of the DenseBlock\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        \"\"\"\n",
    "        Function for initializing the DenseBlock\n",
    "        Args:\n",
    "            num_layers (int) - number of layers in the block\n",
    "            num_input_features (int) - number of input features\n",
    "            bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "                (i.e. bn_size * k features in the bottleneck layer)\n",
    "            growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "            drop_rate (float) - dropout rate after each dense layer\n",
    "            memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower.\n",
    "        \"\"\"\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate,\n",
    "                                growth_rate=growth_rate,\n",
    "                                bn_size=bn_size,\n",
    "                                drop_rate=drop_rate,\n",
    "                                memory_efficient=memory_efficient) # create a dense layer\n",
    "            self.add_module('denselayer%d' % (i + 1), layer) # add the dense layer to the DenseBlock\n",
    "\n",
    "        def forward(self, init_features):\n",
    "            \"\"\"\n",
    "            Function for forward pass of the DenseBlock\n",
    "            Args:\n",
    "                init_features (torch.autograd.Variable) - input to the layer\n",
    "            \"\"\"\n",
    "            features = [init_features] # store the input as a list\n",
    "            for name, layer in self.items():\n",
    "                new_features = layer(features) # apply the dense layer\n",
    "                features.append(new_features) # append the output of the dense layer to the list\n",
    "            return torch.cat(features, 1) # concatenate the list of features and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DenseNet which is a combination of DenseBlocks and Transition Layers\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate = 32, block_config = (6,12,24,16),num_init_features=64, bn_size = 4, drop_rate = 0, num_classes = 1000,memory_efficient = False):\n",
    "        \"\"\"\n",
    "        Function for initializing the DenseNet\n",
    "        Args:\n",
    "            growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "            block_config (tuple of 4 ints) - how many layers in each pooling block\n",
    "            num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "            bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "                (i.e. bn_size * k features in the bottleneck layer)\n",
    "            drop_rate (float) - dropout rate after each dense layer\n",
    "            num_classes (int) - number of classification classes\n",
    "            memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower.\n",
    "        \"\"\"\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0',nn.Conv2d(3,num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ('norm0',nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0',nn.ReLU(inplace=True)),\n",
    "            ('pool0',nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]\n",
    "        ))\n",
    "\n",
    "        # Add multiple dense blocks based on block configuration array (block_config)\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_input_features=num_features,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate,\n",
    "                                memory_efficient=memory_efficient)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate  # compute the number of features after each dense block\n",
    "\n",
    "            if i != len(block_config) - 1: # if not the last dense block\n",
    "                # Add transition layer between dense blocks to downsample \n",
    "                trans = _Transition(num_input_features=num_features,num_output_features=num_features // 2)\n",
    "                self.features.add_module('trasition%d'%(i+1),trans)\n",
    "                num_features = num_features //2 # compute the number of features after each transition layer\n",
    "        \n",
    "        # Final batch normalisation \n",
    "        self.features.add_module('norm5',nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # initialize the convolutional layers with kaiming normal initialization \n",
    "            elif isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1) # initialize the batch normalization layers with constant weight of 1\n",
    "                nn.init.constant_(m.bias,0) # initialize the batch normalization layers with constant bias of 0 \n",
    "            elif isinstance(m,nn.Linear):\n",
    "                nn.init.constant_(m.bias,0) # initialize the linear layers with constant bias of 0\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Function for forward pass of the DenseNet\n",
    "        Args:\n",
    "            x (torch.autograd.Variable) - input to the layer\n",
    "        \"\"\"\n",
    "        features = self.features(x) # apply the features to the input\n",
    "        out = F.relu(features,inplace=True) # apply relu activation\n",
    "        out = F.adaptive_avg_pool2d(out,(1,1)).view(features.size(0),-1) # apply adaptive average pooling\n",
    "        out = self.classifier(out) # apply the linear layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DenseNet 121 using the DenseNet class\n",
    "\n",
    "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs):\n",
    "    \"\"\"\n",
    "    Function for initializing the DenseNet\n",
    "    Args:\n",
    "        arch (str) - name of the architecture\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (tuple of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        pretrained (bool) - If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool) - If True, displays a progress bar of the download to stderr\n",
    "        **kwargs (dict) - Additional arguments  \n",
    "    \"\"\"\n",
    "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
    "    return model\n",
    "\n",
    "def densenet121(pretrained=False, progress=True, **kwargs):\n",
    "    return _densenet(\"DenseNet121\",32,(6,12,24,16),64,pretrained,progress,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab93b1e3561ecd12f61fab0692a47576cab87e52760ee86e43bbae6bac2de0a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
